import torch

from mr_gen.model.utils.types import NonlinearityType


def mlp_mixer_block_argments(
    hidden_size: int,
    num_layer: int = 1,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bottleneck_size: int = None,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
) -> dict:
    return {
        "hidden_size": hidden_size,
        "num_layer": num_layer,
        "nonlinearity": nonlinearity,
        "residual": residual,
        "residual_layer_norm": residual_layer_norm,
        "bottleneck_size": bottleneck_size,
        "bias": bias,
        "device": device,
        "dtype": dtype,
    }


def gru_mixer_block_argments(
    hidden_size: int,
    num_layers: int = 1,
    dropout: float = 0.0,
    batch_first: bool = True,
    bidirectional: bool = False,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bottleneck_size: int = None,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
) -> dict:
    return {
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "dropout": dropout,
        "batch_first": batch_first,
        "bidirectional": bidirectional,
        "nonlinearity": nonlinearity,
        "residual": residual,
        "residual_layer_norm": residual_layer_norm,
        "bottleneck_size": bottleneck_size,
        "bias": bias,
        "device": device,
        "dtype": dtype,
    }


def lstm_mixer_block_argments(
    hidden_size: int,
    num_layers: int = 1,
    dropout: float = 0.0,
    batch_first: bool = True,
    bidirectional: bool = False,
    proj_size: int = 0,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bottleneck_size: int = None,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
) -> dict:
    return {
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "dropout": dropout,
        "batch_first": batch_first,
        "bidirectional": bidirectional,
        "proj_size": proj_size,
        "nonlinearity": nonlinearity,
        "residual": residual,
        "residual_layer_norm": residual_layer_norm,
        "bottleneck_size": bottleneck_size,
        "bias": bias,
        "device": device,
        "dtype": dtype,
    }


def mha_mixer_block_argments(
    hidden_size: int,
    num_heads: int,
    num_layers: int = 1,
    dropout: float = 0.0,
    batch_first: bool = True,
    add_bias_kv: bool = False,
    add_zero_attn: bool = False,
    kdim: int = None,
    vdim: int = None,
    max_context_len: int = 125,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bottleneck_size: int = None,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
) -> dict:
    return {
        "hidden_size": hidden_size,
        "num_heads": num_heads,
        "num_layers": num_layers,
        "dropout": dropout,
        "batch_first": batch_first,
        "add_bias_kv": add_bias_kv,
        "add_zero_attn": add_zero_attn,
        "kdim": kdim,
        "vdim": vdim,
        "max_context_len": max_context_len,
        "nonlinearity": nonlinearity,
        "residual": residual,
        "residual_layer_norm": residual_layer_norm,
        "bottleneck_size": bottleneck_size,
        "bias": bias,
        "device": device,
        "dtype": dtype,
    }


def mlp_mixer_layerd_argments(
    hidden_size: int,
    input_projection: bool = False,
    input_projection_size: int = None,
    output_projection: bool = False,
    output_projection_size: int = None,
    num_layerd: int = 1,
    num_internal_layer: int = 1,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bottleneck_size: int = None,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
) -> dict:
    return {
        "hidden_size": hidden_size,
        "input_projection": input_projection,
        "input_projection_size": input_projection_size,
        "output_projection": output_projection,
        "output_projection_size": output_projection_size,
        "num_layerd": num_layerd,
        "num_internal_layer": num_internal_layer,
        "nonlinearity": nonlinearity,
        "residual": residual,
        "residual_layer_norm": residual_layer_norm,
        "bottleneck_size": bottleneck_size,
        "bias": bias,
        "device": device,
        "dtype": dtype,
    }


def gru_mixer_layerd_argments(
    hidden_size: int,
    input_projection: bool = False,
    input_projection_size: int = None,
    output_projection: bool = False,
    output_projection_size: int = None,
    num_layerd: int = 1,
    num_internal_layer: int = 1,
    dropout: float = 0.0,
    batch_first: bool = True,
    bidirectional: bool = False,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bottleneck_size: int = None,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
) -> dict:
    return {
        "hidden_size": hidden_size,
        "input_projection": input_projection,
        "input_projection_size": input_projection_size,
        "output_projection": output_projection,
        "output_projection_size": output_projection_size,
        "num_layerd": num_layerd,
        "num_internal_layer": num_internal_layer,
        "dropout": dropout,
        "batch_first": batch_first,
        "bidirectional": bidirectional,
        "nonlinearity": nonlinearity,
        "residual": residual,
        "residual_layer_norm": residual_layer_norm,
        "bottleneck_size": bottleneck_size,
        "bias": bias,
        "device": device,
        "dtype": dtype,
    }


def lstm_mixer_layerd_argments(
    hidden_size: int,
    input_projection: bool = False,
    input_projection_size: int = None,
    output_projection: bool = False,
    output_projection_size: int = None,
    num_layerd: int = 1,
    num_internal_layer: int = 1,
    dropout: float = 0.0,
    batch_first: bool = True,
    bidirectional: bool = False,
    proj_size: int = 0,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bottleneck_size: int = None,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
) -> dict:
    return {
        "hidden_size": hidden_size,
        "input_projection": input_projection,
        "input_projection_size": input_projection_size,
        "output_projection": output_projection,
        "output_projection_size": output_projection_size,
        "num_layerd": num_layerd,
        "num_internal_layer": num_internal_layer,
        "dropout": dropout,
        "batch_first": batch_first,
        "bidirectional": bidirectional,
        "proj_size": proj_size,
        "nonlinearity": nonlinearity,
        "residual": residual,
        "residual_layer_norm": residual_layer_norm,
        "bottleneck_size": bottleneck_size,
        "bias": bias,
        "device": device,
        "dtype": dtype,
    }


def mha_mixer_layerd_argments(
    hidden_size: int,
    input_projection: bool = False,
    input_projection_size: int = None,
    self_attention: bool = False,
    output_projection: bool = False,
    output_projection_size: int = None,
    num_heads: int = 1,
    dropout: float = 0.0,
    batch_first: bool = True,
    add_bias_kv: bool = False,
    add_zero_attn: bool = False,
    kdim: int = None,
    vdim: int = None,
    max_context_len: int = 125,
    num_layerd: int = 1,
    num_internal_layer: int = 1,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bottleneck_size: int = None,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
) -> dict:
    return {
        "hidden_size": hidden_size,
        "input_projection": input_projection,
        "input_projection_size": input_projection_size,
        "self_attention": self_attention,
        "output_projection": output_projection,
        "output_projection_size": output_projection_size,
        "num_heads": num_heads,
        "dropout": dropout,
        "batch_first": batch_first,
        "add_bias_kv": add_bias_kv,
        "add_zero_attn": add_zero_attn,
        "kdim": kdim,
        "vdim": vdim,
        "max_context_len": max_context_len,
        "num_layerd": num_layerd,
        "num_internal_layer": num_internal_layer,
        "nonlinearity": nonlinearity,
        "residual": residual,
        "residual_layer_norm": residual_layer_norm,
        "bottleneck_size": bottleneck_size,
        "bias": bias,
        "device": device,
        "dtype": dtype,
    }


def feedforward_block_argments(
    hidden_size: int,
    bottleneck_size: int = None,
    output_size: int = None,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
) -> dict:
    return {
        "hidden_size": hidden_size,
        "bottleneck_size": bottleneck_size,
        "output_size": output_size,
        "nonlinearity": nonlinearity,
        "residual": residual,
        "residual_layer_norm": residual_layer_norm,
        "bias": bias,
        "device": device,
        "dtype": dtype,
    }


def mixer_layerd_argments_select(
    mixer_type: str,
    hidden_size: int,
    input_projection: bool = False,
    input_projection_size: int = None,
    self_attention: bool = False,
    output_projection: bool = False,
    output_projection_size: int = None,
    num_heads: int = 1,
    dropout: float = 0.0,
    batch_first: bool = True,
    bidirectional: bool = False,
    proj_size: int = 0,
    add_bias_kv: bool = False,
    add_zero_attn: bool = False,
    kdim: int = None,
    vdim: int = None,
    max_context_len: int = 125,
    num_layerd: int = 1,
    num_internal_layer: int = 1,
    nonlinearity: NonlinearityType = None,
    residual: bool = False,
    residual_layer_norm: bool = False,
    bottleneck_size: int = None,
    bias: bool = True,
    device: torch.device = None,
    dtype: torch.dtype = None,
):
    if mixer_type == "mlp":
        return mlp_mixer_layerd_argments(
            hidden_size=hidden_size,
            input_projection=input_projection,
            input_projection_size=input_projection_size,
            output_projection=output_projection,
            output_projection_size=output_projection_size,
            num_layerd=num_layerd,
            num_internal_layer=num_internal_layer,
            nonlinearity=nonlinearity,
            residual=residual,
            residual_layer_norm=residual_layer_norm,
            bottleneck_size=bottleneck_size,
            bias=bias,
            device=device,
            dtype=dtype,
        )
    elif mixer_type == "gru":
        return gru_mixer_layerd_argments(
            hidden_size=hidden_size,
            input_projection=input_projection,
            input_projection_size=input_projection_size,
            output_projection=output_projection,
            output_projection_size=output_projection_size,
            num_layerd=num_layerd,
            num_internal_layer=num_internal_layer,
            dropout=dropout,
            batch_first=batch_first,
            bidirectional=bidirectional,
            nonlinearity=nonlinearity,
            residual=residual,
            residual_layer_norm=residual_layer_norm,
            bottleneck_size=bottleneck_size,
            bias=bias,
            device=device,
            dtype=dtype,
        )
    elif mixer_type == "lstm":
        return lstm_mixer_layerd_argments(
            hidden_size=hidden_size,
            input_projection=input_projection,
            input_projection_size=input_projection_size,
            output_projection=output_projection,
            output_projection_size=output_projection_size,
            num_layerd=num_layerd,
            num_internal_layer=num_internal_layer,
            dropout=dropout,
            batch_first=batch_first,
            bidirectional=bidirectional,
            proj_size=proj_size,
            nonlinearity=nonlinearity,
            residual=residual,
            residual_layer_norm=residual_layer_norm,
            bottleneck_size=bottleneck_size,
            bias=bias,
            device=device,
            dtype=dtype,
        )
    elif mixer_type == "mha":
        return mha_mixer_layerd_argments(
            hidden_size=hidden_size,
            input_projection=input_projection,
            input_projection_size=input_projection_size,
            self_attention=self_attention,
            output_projection=output_projection,
            output_projection_size=output_projection_size,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=batch_first,
            add_bias_kv=add_bias_kv,
            add_zero_attn=add_zero_attn,
            kdim=kdim,
            vdim=vdim,
            max_context_len=max_context_len,
            num_layerd=num_layerd,
            num_internal_layer=num_internal_layer,
            nonlinearity=nonlinearity,
            residual=residual,
            residual_layer_norm=residual_layer_norm,
            bottleneck_size=bottleneck_size,
            bias=bias,
            device=device,
            dtype=dtype,
        )
